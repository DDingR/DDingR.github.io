---
layout: post
title: DQN
---

daunting: 기력을 꺽다; 학습을 잘 안되게 한다는 의미로 사용

alleviate: 경감하다; 문제를 줄이는 의미로 사용

elapsed: 경과한; 타임스텝이 경과된 후 delay의미로 사용]

discriminate: 분간

# Abstract

- Successfully executed reinforcement learning from directly high-dim sensory input (image) to control policy (estimates of future rewards)
- model is consisted with CNN trained with a variant of Q-learning
- applied to Atari games with out adjusrment of the architecture

# 1. Introduction

- learning from high-dim sensory input (vision, speech) is long standind challenges in RL
- perfromances strongly depend on quality of feature representation
(conventionally successful RL was executed with hand-crafted features)
- in field of CV and speech recognition, breakthrough methods are developed using CNN, RNN and others
- there are several reasons that cannot apply these methods on RL
    - RL learns from scalar reward signal (even sparse, noisy and delay)
    - samples are highly dependent each others on sequencial states
    (furthermore, data distribution changes as algorithm learns new behavior
    정책이 업데이트 됨에 따라서 관찰되는 state 분포가 달라짐
    → problem for deep learning methid assuming fixed underlying distribution
- used stochastic gradient descent from experience replay mechanism; randomly sampling
- objective: build a model that is able to learn successfully with as possible as many games without any previous information

# 2. Background

- by obseve with raw pixel vector on screen, cannot observe entire state of current state
→ needed to consider as distinct finite MDP sequence
- by *value iteration* as $i\to \infty$, $Q_i\to Q^*$(Optimal action value fuction)
however, each sequence (state) needs each Q without genernalisation, usually assume Q function approximator (linear or non-linear; NN) → $Q=Q(s,a;\theta)$
- loss function defined as MSE of target and Q

$$
L_i(\theta_i)=\mathbb E_{s,a\sim\rho(\cdot)}

\left[(y_i-Q(s,a;\theta_i))^2\right],

\quad

y_i=\mathbb E[r+\gamma\text{max}_{a'} Q(s',a';\theta_{i-1})|s,a]
$$

- target is non-fixed; notice the subscript $i-1$
it is remarkable difference between regular Deep Learning that uses fixed target (lable)
- $i-1$ means the seperated network from current network updated every time-step
- by diffenrential loss function we can obtain below equation

$$
\nabla_{\theta_i}L_i(\theta_i)=\mathbb E_{s,a\sim\rho(\cdot);s'\sim\epsilon}\left[\left(
r+\gamma\max_{a'}Q(s',a';\theta_{i-1})-Q(s,a,;\theta_i)
\right)\nabla_{\theta_i}Q(s,a;\theta_i)\right]
$$

- rather than compute full expectation of above equation, to optimize with SGD is more computationally effcient
if weights are updated every steps and expectation be replaced by single sample → Q-learning
- Notice; model-free, off-policy (epsilon-greedy action)

# 3. Related Work

- previous works used autoencoders to lower dimension fo input
- 무슨소리지?
    
    More recently, there has been a revival of interest in combining deep learning with reinforcement learning. Deep neural networks have been used to estimate the environment E; restricted Boltzmann machines have been used to estimate the value function [21]; or the policy [9]. In addition, the divergence issues with Q-learning have been partially addressed by gradient temporal-difference methods. These methods are proven to converge when evaluating a ﬁxed policy with a nonlinear function approximator [14]; or when learning a control policy with linear function approximation using a restricted variant of Q-learning [15]. However, these methods have not yet been extended to nonlinear control.
    

# 4. Deep Reinforcement Learning

- key feature is applying SGD to reinforcement learning

![_config.yml](/images/paper_review_images/DQN_pseudo.png)


- advantages of DQN
    - with experience replay, data could be used in many weight updates; data efficiency
    - randomly sampling from replay breaks correlations of sequencial datas
    reduce probability of divergence
    - learning is non-dominant by current trained parameters; Off-policy
    less probability of local minimum and divergence
    in case of this, the sample from replay is not generated by current parameters
- limits
    - cannot differentiate important transition
    - overwrite recent trasition (sample) on replay
    - same importance on all trasition in replay; same sampling size

## 4.1 Preprocessing and Model Architecture

described as $\phi$ function on Algorithm 1

- from input image (w * b * c)
gray scaling (210 * 160 * 128 → 210 * 160 * 1)
crop to square for 2D CNN (210 * 160 * 1 → 84 * 84 * 1)
stack last 4 frame (84 * 84 * 1 → 84 * 84 * 4)
- to build each Q function for each action for certain state require high computational cost
- in this architecture, defined a single Q approximator network has output layer node corresponding number of actions (to return each Q-value from input; state)