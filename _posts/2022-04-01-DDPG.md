# DDPG

intractable: 다루기 힘든; 성공적으로 학습하기 힘들다는 의미로 사용

# Abstract

- apply Deep Q-Learning to continous action domain
- actor-critic, model-free based on deterministic policy gradient
- end-to-end; raw pixel input

# 1. Introduction

- with DQN, performance from raw pixel input overcome human-level
- however, DQN only be usable to low-dim, discrete action domain
- limitation of dicretization of continous action domain
    - curse of dimensionality; number of actions increases exponentially with the nuber of DOF
    - naive discretization might throws away key information of action domain structure
- use insights from DQN
    - replay buffer; off-policy; SGD (less correlations of samples)
    - target Q network for consistent targets during TD
- able to learn competitive policies from low-dim observations (e.g. cartesian coords or joint angle) or raw pixel input

# 2. Background

# 3. Algorithm

- based on DPG algorithm $a=\mu(s|\theta^\mu)$ and actor-critic

$$
\nabla_{\theta^\mu}J\approx

\mathbb E_{s_t\sim\rho^\beta}

[\nabla_{\theta^\mu}Q(s,a|\theta^Q)|_{s=s_t,a=\mu(s_t|\theta^\mu)}]

\\

=\mathbb E_{s_t\sim\rho^\beta}

[\nabla_aQ(s,a|\theta^Q)|_{s=s_t,a=\mu(s_t)}\nabla_{\theta_\mu}\mu(s|\theta^\mu)|_{s=s_t}]
$$

- non-linear approximator no longer guarantees convergence, but needed for large size of state and action
- 기존 네트워크로 NFQCA를 DPG랑 비교했는데 이는 NN approximator와 batch learning을 사용.  
한편 DPG는 Differentiable function approximator와 mini-batch 사용
- to use NN in RL, applied replay buffer like DQN to cut off the dependencies between samples
- update target networks in “soft” way $\theta'\leftarrow\tau\theta+(1-\tau)\theta'$  
can make the learning like supervised learning (quite stable but slow)
- used batch normalization 자세한건 링크 참고

[[Deep Learning] Batch Normalization (배치 정규화)](https://eehoeskrap.tistory.com/430)

- to ensure exploration, added noise (Ornstein-Uhlenbeck process)

$$
\mu'(s_t)=\mu(s_t|\theta^\mu_t)+N\quad N:\text{Ornstein-Uhlenbeck process}
$$

# 4. Results
![_config.yml](/images/paper_review_images/DDPG_pseudo.png)
